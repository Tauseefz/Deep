{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n",
      "41.621148109436035\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets \n",
    "from math import exp\n",
    "import time\n",
    "start = time.time()\n",
    "# the logistic function\n",
    "def logistic_func(theta, x):\n",
    "    t = x.dot(theta)\n",
    "    g = np.zeros(t.shape)\n",
    "    # split into positive and negative to improve stability\n",
    "    g[t>=0.0] = 1.0 / (1.0 + np.exp(-t[t>=0.0])) \n",
    "    g[t<0.0] = np.exp(t[t<0.0]) / (np.exp(t[t<0.0])+1.0)\n",
    "    return g\n",
    "\n",
    "# function to compute log-likelihood\n",
    "def neg_log_like(theta, x, y):\n",
    "    g = logistic_func(theta,x)\n",
    "    return -sum(np.log(g[y>0.5])) - sum(np.log(1-g[y<0.5]))\n",
    "\n",
    "# function to compute the gradient of the negative log-likelihood\n",
    "def log_grad(theta, x, y):\n",
    "    g = logistic_func(theta,x)\n",
    "    return -x.T.dot(y-g)\n",
    "def hessain(theta , x , y):\n",
    "    h= logistic_func(theta , x)\n",
    "    return x.T.dot(theta).dot(x)\n",
    "# implementation of gradient descent for logistic regression\n",
    "def grad_desc(theta, x, y, alpha, tol, maxiter):\n",
    "    nll_vec = []\n",
    "    nll_vec.append(neg_log_like(theta, x, y))\n",
    "    nll_delta = 2.0*tol\n",
    "    iter = 0\n",
    "    while (nll_delta > tol) and (iter < maxiter):\n",
    "        theta = theta - (alpha * log_grad(theta, x, y)) \n",
    "        nll_vec.append(neg_log_like(theta, x, y))\n",
    "        nll_delta = nll_vec[-2]-nll_vec[-1]\n",
    "        iter += 1\n",
    "    print(iter)\n",
    "    return theta, np.array(nll_vec)\n",
    "\n",
    "# function to compute output of LR classifier\n",
    "def lr_predict(theta,x):\n",
    "    # form Xtilde for prediction\n",
    "    shape = x.shape\n",
    "    Xtilde = np.zeros((shape[0],shape[1]+1))\n",
    "    Xtilde[:,0] = np.ones(shape[0])\n",
    "    Xtilde[:,1:] = x\n",
    "    return logistic_func(theta,Xtilde)\n",
    "\n",
    "## Generate dataset    \n",
    "np.random.seed(2017) # Set random seed so results are repeatable\n",
    "x,y = datasets.make_blobs(n_samples=100000,n_features=2,centers=2,cluster_std=6.0)\n",
    "\n",
    "## build classifier\n",
    "# form Xtilde\n",
    "shape = x.shape\n",
    "xtilde = np.zeros((shape[0],shape[1]+1))\n",
    "xtilde[:,0] = np.ones(shape[0])\n",
    "xtilde[:,1:] = x\n",
    "\n",
    "# Initialize theta to zero\n",
    "theta = np.zeros(shape[1]+1)\n",
    "\n",
    "# Run gradient descent\n",
    "alpha = 0.0000009\n",
    "tol = 1e-3\n",
    "maxiter = 10000\n",
    "theta,cost = grad_desc(theta,xtilde,y,alpha,tol,maxiter)\n",
    "\n",
    "## Plot the decision boundary. \n",
    "# Begin by creating the mesh [x_min, x_max]x[y_min, y_max].\n",
    "h = .02  # step size in the mesh\n",
    "x_delta = (x[:, 0].max() - x[:, 0].min())*0.05 # add 5% white space to border\n",
    "y_delta = (x[:, 1].max() - x[:, 1].min())*0.05\n",
    "x_min, x_max = x[:, 0].min() - x_delta, x[:, 0].max() + x_delta\n",
    "y_min, y_max = x[:, 1].min() - y_delta, x[:, 1].max() + y_delta\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = lr_predict(theta,np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Create color maps\n",
    "#cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "#cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "#plt.figure()\n",
    "#plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "## Plot the training points\n",
    "#plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold)\n",
    "#print(iter)\n",
    "## Show the plot\n",
    "#plt.xlim(xx.min(), xx.max())\n",
    "#plt.ylim(yy.min(), yy.max())\n",
    "#plt.title(\"Logistic regression classifier\")\n",
    "#plt.show()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
